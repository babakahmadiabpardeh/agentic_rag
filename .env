# .env

# Specify the Ollama models to use for embedding and language generation
EMBEDDING_MODEL=nomic-embed-text
LLM_MODEL=llama3.2
CHROMA_COLLECTION_NAME=rag-chroma-collection
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
RETRIEVER_K=4
SYSTEM_PROMPT="Answer the user's questions based on the below context. Also provide the source document for each piece of information:\n\n{context}"
RETRIEVER_PROMPT="Given the above conversation, generate a search query to look up in order to get information relevant to the conversation"